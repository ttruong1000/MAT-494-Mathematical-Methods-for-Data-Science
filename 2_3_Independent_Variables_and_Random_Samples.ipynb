{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYMCoBgWTm3J3Qy0VwGDkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttruong1000/MAT-494-Mathematical-Methods-for-Data-Science/blob/main/2_3_Independent_Variables_and_Random_Samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.3 - Independent Variables and Random Samples**"
      ],
      "metadata": {
        "id": "boh8Gmhh4aTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.0 - Python Libraries for Independent Variables and Random Samples**"
      ],
      "metadata": {
        "id": "cFMdAWCQEh1b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3c-A0UNaEov4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.1 - Joint Probability Distributions**"
      ],
      "metadata": {
        "id": "HGir1T1eDr2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.1 - Joint Probability Mass Function for Two Discrete Random Variables"
      ],
      "metadata": {
        "id": "HekqDZtSE8i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X$ and $Y$ be two discrete random variables defined on the sample space $S$ of an experiment. The joint probability mass function $p(x, y)$ is defined for each pair of numbers $(x, y)$ by\n",
        "\\begin{equation*}\n",
        "  p(x, y) = P(X = x \\text{ and } Y = y)\n",
        "\\end{equation*}\n",
        "It must be the case that $p(x, y) \\geq 0$ nad $\\displaystyle\\sum_x\\sum_yp(x, y) = 1$."
      ],
      "metadata": {
        "id": "dXSY7Fj2E80l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.2 - Marginal Probability Mass Function for Two Discrete Random Variables"
      ],
      "metadata": {
        "id": "zcgbyJObM2ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The marginal probability mass function of $X$, denoted by $p_X(x)$, is given by\n",
        "\\begin{equation*}\n",
        "  p_X(x) = \\sum_{y:p(x, y) > 0} p(x, y) \\quad \\text{ for each possible value of $x$}\n",
        "\\end{equation*}\n",
        "The marginal probability mass function of $Y$, denoted by $p_Y(y)$, is given by\n",
        "\\begin{equation*}\n",
        "  p_X(x) = \\sum_{x:p(x, y) > 0} p(x, y) \\quad \\text{ for each possible value of $y$}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "TDr_8QyZM3AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.3 - Joint Probability Density Function for Two Continuous Random Variables"
      ],
      "metadata": {
        "id": "VA_iocyBNWmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X$ and $Y$ be two continuous random variables. A joint probability density function $f(x, y)$ for these two variables is a function satisfying $f(x, y) \\geq 0$ and $\\int_{-\\infty}^{\\infty} f(x, y) \\ dxdy = 1$. Then, for any two-dimensional set $A$,\n",
        "\\begin{equation*}\n",
        "  P[(X, Y) \\in A] = \\int\\int_A f(x, y) \\ dA\n",
        "\\end{equation*}\n",
        "In particular, if $A$ is the two-dimensional rectangle $\\{(x, y) : a \\leq x \\leq b, c \\leq y \\leq d\\}$, then\n",
        "\\begin{equation*}\n",
        "  P[(X, Y) \\in A] = P[a \\leq x \\leq b, c \\leq y \\leq d] = \\int_a^b\\int_c^d f(x, y) \\ dydx\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "Bt2luCI0NW75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.4 - Marginal Probability Mass Function for Two Continuous Random Variables"
      ],
      "metadata": {
        "id": "C2RZIXNkNia5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The marginal probability density functions of $X$ and $Y$, denoted by $f_X(x)$ and $f_Y(y)$, respectively, are\n",
        "\\begin{equation*}\n",
        "  f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\ dy \\quad \\text{ for $-\\infty < x < \\infty$}\n",
        "\\end{equation*}\n",
        "\\begin{equation*}\n",
        "  f_Y(y) = \\int_{-\\infty}^{\\infty} f(x, y) \\ dx \\quad \\text{ for $-\\infty < y < \\infty$}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "yQYyUeN1Niyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.5 - Indepedent Random Variables"
      ],
      "metadata": {
        "id": "4UByhBNZPQW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two random variables $X$ and $Y$ are said to be independent if for every pair $(x, y)$,\n",
        "\\begin{equation*}\n",
        "  p(x, y) = p_X(x)p_Y(y) \\quad \\text{ where $X$ and $Y$ are discrete}\n",
        "\\end{equation*}\n",
        "or\n",
        "\\begin{equation*}\n",
        "  f(x, y) = f_X(x)f_Y(y) \\quad \\text{ where $X$ and $Y$ are continuous}\n",
        "\\end{equation*}\n",
        "If the above is not satisfied for all $(x, y)$, then $X$ and $Y$ are dependent."
      ],
      "metadata": {
        "id": "BKsS8r6GPQ-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.6 - Joint Distribution of $N$ Random Variables"
      ],
      "metadata": {
        "id": "GXlFa2TiP4Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $X_1, X_2, \\ldots, X_n$ are all discrete random variables, the joint PMF of the variables is the function\n",
        "\\begin{equation*}\n",
        "  p(x_1, x_2, \\ldots, x_n) = P(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n)\n",
        "\\end{equation*}\n",
        "If the variables are continuous, the joint PDF of $X_1, X_2, \\ldots, X_n$ is the function $f(x_1, x_2, \\ldots, x_n)$ such that for any $n$ intervals $[a_1, b_1], [a_2, b_2], \\ldots, [a_n, b_n]$,\n",
        "\\begin{equation*}\n",
        "  P(a_1 \\leq X_1 \\leq b_1, a_2 \\leq X_2 \\leq b_2, \\ldots, a_n \\leq X_n \\leq b_n) = \\int_{a_1}^{b_1} \\int_{a_2}^{b_2} \\cdots \\int_{a_n}^{b_n} f(x_1, x_2, \\ldots, x_n) \\ dx_n\\cdots dx_2dx_1\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "DqbyJnruP4pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.1.7 - Independence of $N$ Random Variables"
      ],
      "metadata": {
        "id": "ZHO9E93kP5GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random variables $X_1, X_2, \\ldots, X_n$ are said to be independent if for every subset $X_{i_1}, X_{i_2}, \\ldots, X_{i_k}$ of the variables ($n$-tuples), the joint PMF or PDF of the subset is equal to the product of the marginal PMF's or PDF's."
      ],
      "metadata": {
        "id": "JEBsQ2GyP5jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.2 - Correlation and Dependence**"
      ],
      "metadata": {
        "id": "UKuMR8ayDsFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.2.1 - Covariance Between Two Random Variables"
      ],
      "metadata": {
        "id": "e5PWP0BAPQoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X$ and $Y$ be jointly distributed random variables with PMF $p(x, y)$ or PDF $f(x, y)$ according to whether the variables are discrete or continuous. The covariance between two random variables $X$ and $Y$ is\n",
        "\\begin{equation*}\n",
        "  \\text{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = \\begin{cases}\n",
        "                \\displaystyle\\sum_x\\sum_y(x - \\mu_X)(y - \\mu_Y)p(x, y)\n",
        "                & \\text{$X, Y$ discrete} \\\\\n",
        "                \\displaystyle\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x - \\mu_X)(y - \\mu_Y)f(x, y) \\ dxdy\n",
        "                & \\text{$X, Y$ continuous} \\\\\n",
        "            \\end{cases}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "6rQuEIgfPRZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.2.2 - Correlation Coefficient"
      ],
      "metadata": {
        "id": "wL14bUn9UUD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient of $X$ and $Y$, denoted by $\\text{Corr}(X, Y)$ $\\rho_{X, Y}$, or $\\rho$, is defined by\n",
        "\\begin{equation*}\n",
        "  \\text{Corr}(X, Y) = \\rho_{X, Y} = \\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X\\sigma_Y}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "kNiJdYULUUSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Proposition 2.3.2.3 - Properties of the Correlation Coefficient"
      ],
      "metadata": {
        "id": "xHnGfmZWUVAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation coefficients have the following properties.\n",
        "- If $X$ and $Y$ are independent, then $\\rho = 0$, but $\\rho = 0$ does not imply independence.\n",
        "- $|\\rho| \\leq 1$, $\\rho = -1$ or $\\rho = 1$ if $Y = aX + b$ for some numbers $a$ and $b$ with $a \\neq 0$."
      ],
      "metadata": {
        "id": "6ziAKIuwUVO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Proposition 2.3.2.4 - Correlation Coefficient for Samples"
      ],
      "metadata": {
        "id": "vRTVx32SUXec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient for samples has the following properties.\n",
        "- $r_{xy} = \\frac{s_{xy}}{s_xs_y}$, where the sample covariance is\n",
        "\\begin{equation*}\n",
        "  s_{xy} = \\frac{1}{n - 1}\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n",
        "\\end{equation*}\n",
        "and the sample standard deviation is\n",
        "\\begin{equation*}\n",
        "  s_x = \\sqrt{\\frac{1}{n - 1}\\sum_{i = 1}^n(x_i - \\overline{x})^2}\n",
        "\\end{equation*}\n",
        "- If $y = ax + b$, then $r_{xy} = -1$ or $r_{xy} = 1$ if $a$ is negative or positive, respectively."
      ],
      "metadata": {
        "id": "oGsVPh8zUXs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.3 - Random Samples**"
      ],
      "metadata": {
        "id": "YPmtvE0CDsUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.3.1 - Random Sample of Size $n$"
      ],
      "metadata": {
        "id": "nwUv9GrCUZJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random variables $X_1, X_2, \\ldots, X_n$ are said to form a (simple) random sample of size $n$ if\n",
        "- the $X_i$'s are independent random variables\n",
        "- every $X_i$ has the same probability distribution"
      ],
      "metadata": {
        "id": "8P_THXp0UZWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Propostion 2.3.3.2 - Properties of Random Samples from a Distribution"
      ],
      "metadata": {
        "id": "bNlP3e5pUZld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a distribution with mean value $\\mu$ and standard deviation $\\sigma$. Then,\n",
        "- $E(\\overline{X}) = \\mu_{\\overline{X}} = \\mu$\n",
        "- $V(\\overline{X}) = \\sigma_{\\overline{X}}^2 = \\frac{\\sigma^2}{n}$ and $\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}$"
      ],
      "metadata": {
        "id": "QL3M3uE8UZ0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 2.3.3.3 - The Central Limit Theorem (CLT)"
      ],
      "metadata": {
        "id": "fz4rOBTwUaFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$. Then, if $n$ is sufficiently large, $\\overline{X}$ has approximately a normal distribution with $\\mu_{\\overline{X}} = \\mu$ and $\\sigma_{\\overline{X}}^2 = \\frac{\\sigma^2}{n}$, and $T_0$ also has an approximately normal distribution with $\\mu_{T_0} = n\\mu$ and $\\sigma_{T_0}^2 = n\\sigma^2$. The larger the value of $n$, the better the approximation."
      ],
      "metadata": {
        "id": "lJPaV-2iUaVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.4 - References**"
      ],
      "metadata": {
        "id": "99nXbiTzDsjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. MAT 494 Chapter 2 Lecture Notes"
      ],
      "metadata": {
        "id": "ON_MNrdNUam-"
      }
    }
  ]
}