{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf01ygZliKwTjeuQY0byLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttruong1000/MAT-494-Mathematical-Methods-for-Data-Science/blob/main/3_3_Unconstrained_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 - Unconstrained Optimization**"
      ],
      "metadata": {
        "id": "H2B9n10Jrcrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3.0 - Python Libraries for Unconstrained Optimization**"
      ],
      "metadata": {
        "id": "51xERIJ6rpIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize"
      ],
      "metadata": {
        "id": "cMOfhazWtC91"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3.1 - Necessary and Sufficient Conditions of Local Minimizers**"
      ],
      "metadata": {
        "id": "smeq_Xntrhf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.1.1 - Global Minimizer"
      ],
      "metadata": {
        "id": "OJ9oxW6TtDdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$. The point $\\mathbf{x}* \\in \\mathbb{R}^d$ is a global minimizer of $f$ over $\\mathbb{R}^d$ if\n",
        "\\begin{equation*}\n",
        "  f(\\mathbf{x})) \\geq f(\\mathbb{x}^*)\n",
        "\\end{equation*}\n",
        "for all $\\mathbf{x} \\in \\mathbb{R}^d$."
      ],
      "metadata": {
        "id": "Y3O4E96f5zma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.1.2 - (Strict) Local Minimizer"
      ],
      "metadata": {
        "id": "NCtt3F9U6Ifi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$. The point $\\mathbf{x}* \\in \\mathbb{R}^d$ is a local minimizer of $f$ over $\\mathbb{R}^d$ if there is a $\\delta > 0$ such that\n",
        "\\begin{equation*}\n",
        "  f(\\mathbf{x})) \\geq f(\\mathbb{x}^*)\n",
        "\\end{equation*}\n",
        "for all $\\mathbf{x} \\in B_{\\delta}(\\mathbf{x}^*) \\setminus \\{\\mathbf{x}^*\\}$. If the inequality is strict, we say that $\\mathbf{x}^*$ is a strict local minimizer. Alternatively, $\\mathbf{x}^*$ is a local minimizer if there is an open ball around $\\mathbf{x}^*$ where it attains the minimum value. "
      ],
      "metadata": {
        "id": "s4DjAfBu6I0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.1.3 - Descent Direction"
      ],
      "metadata": {
        "id": "KofOQTfH6Nhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$. A vector $\\mathbf{v}$ is a descent direction for $f$ at $\\mathbf{x}_0$ if there is an $\\alpha^* > 0$ such that\n",
        "\\begin{equation*}\n",
        "  f(\\mathbf{x}_0 + \\alpha\\mathbf{v}) < f(\\mathbf{x}_0)\n",
        "\\end{equation*}\n",
        "for all $\\alpha \\in (0, \\alpha^*)$."
      ],
      "metadata": {
        "id": "luxZ80RQ6NwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.1.4 - Descent Direction and Directional Derivatives"
      ],
      "metadata": {
        "id": "ie8c5bxy6N93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $\\mathbf{x}_0$. A vector $\\mathbf{v}$ is a descent direction for $f$ at $\\mathbf{x}_0$ if\n",
        "\\begin{equation*}\n",
        "  \\frac{\\partial f(\\mathbf{x}_0)}{\\partial \\mathbf{v}} = \\nabla f(\\mathbf{x}_0)^T\\mathbf{v} < 0\n",
        "\\end{equation*}\n",
        "that is, the directional derivative of $f$ at $\\mathbf{x}_0$ in the direction of $\\mathbf{v}$ is negative."
      ],
      "metadata": {
        "id": "S_uip-S_6PG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.1.5 - Existence of a Descent Direction"
      ],
      "metadata": {
        "id": "w0FsG60R6PWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $\\mathbf{x}_0$ and assume that $\\nabla f(\\mathbf{x}_0) \\neq 0$. Then, $f$ has a descent direction at $\\mathbf{x}_0$."
      ],
      "metadata": {
        "id": "I6nmGnuh6PzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.1.6 - First-Order Necessary Condition for Local Minimizers"
      ],
      "metadata": {
        "id": "910RjeHd6P-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable on $\\mathbb{R}^d$. If $\\mathbf{x}_0$ is a local minimizer, then $\\nabla f(\\mathbf{x}_0) = 0$."
      ],
      "metadata": {
        "id": "ol86HKUD6QJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.1.7 - Positive Semi-Definite Matrix (PSD)"
      ],
      "metadata": {
        "id": "L-M6ceds6QcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A square symmetric $d \\times d$ matrix $H$ is positive semi-definite (PSD) if $\\mathbf{x}^TH\\mathbf{x} \\geq 0$ for any $\\mathbf{x} \\in \\mathbb{R}^d$."
      ],
      "metadata": {
        "id": "713WbWzk6QnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.1.8 - Second-Order Necessary Condition for Local Minimizers"
      ],
      "metadata": {
        "id": "rblMqLI36Qz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable on $\\mathbb{R}^d$. If $\\mathbf{x}_0$ is a local minimizer, then $\\mathbf{H}_f(\\mathbf{x}_0)$ is PSD."
      ],
      "metadata": {
        "id": "70P671vT8vjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.1.9 - Second-Order Sufficient Condition for Local Minimizers"
      ],
      "metadata": {
        "id": "lBXt3Ikv8vwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable on $\\mathbb{R}^d$. If $\\nabla f(\\mathbf{x}_0) = 0$ and $\\mathbf{H}_f(\\mathbf{x}_0)$ is positive definite, then $\\mathbf{x}_0$ is a strict local minimizer."
      ],
      "metadata": {
        "id": "h0NTYW0B8v7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3.2 - Convexity and Global Minimizers**"
      ],
      "metadata": {
        "id": "MFu_TKQFrs9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.2.1 - Convex Sets\n"
      ],
      "metadata": {
        "id": "HpkAZxoltD3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A set $D \\subseteq \\mathbb{R}^d$ is convex if for all $\\mathbf{x}, \\mathbf{y} \\in D$ and for all $\\alpha \\in [0, 1]$, $(1 - \\alpha)\\mathbf{x} + \\alpha\\mathbf{y} \\in D$."
      ],
      "metadata": {
        "id": "Pc0eFd8f9l4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definition 3.3.2.2 - Convex Functions"
      ],
      "metadata": {
        "id": "-rPtU09_9mOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function $f: \\mathbb{R}^d \\to \\mathbb{R}$ is convex if, for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ and all $\\alpha \\in [0, 1]$,\n",
        "\\begin{equation*}\n",
        "  f((1 - \\alpha)\\mathbf{x} + \\alpha\\mathbf{y}) \\leq (1 - \\alpha)f(\\mathbf{x}) + \\alpha f(\\mathbf{y})\n",
        "\\end{equation*}\n",
        "More generally, a function $f: D \\to \\mathbb{R}$ over a convex domain $D \\subseteq \\mathbb{R}^d$ is convex if the definition above holds over all $\\mathbf{x}, \\mathbf{y} \\in D$."
      ],
      "metadata": {
        "id": "6et0RCf19meJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.2.3 - Affine Functions are Convex"
      ],
      "metadata": {
        "id": "cc77gxZW9mvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$. The function $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b$ is convex."
      ],
      "metadata": {
        "id": "oLoLnxsu9m7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.2.4 - First-Order Convexity Condition"
      ],
      "metadata": {
        "id": "6p_pu1D59nO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbf{R}^d \\to \\mathbb{R}$ be continuously differentiable. Then, $f$ is convex if and only if for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, $f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T(\\mathbf{y} - \\mathbf{x})$."
      ],
      "metadata": {
        "id": "mh43e4JA9neF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.2.5 - Second-Order Convexity Condition"
      ],
      "metadata": {
        "id": "9FgfKGbp9orC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbf{R}^d \\to \\mathbb{R}$ be twice continuously differentiable. Then, $f$ is convex if and only if for all $\\mathbf{x} \\in \\mathbb{R}^d$, $\\mathbf{H}_f(\\mathbf{x})$ is PSD."
      ],
      "metadata": {
        "id": "8Vux1-iQ9okM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.2.6 - Global Minimizers of Convex Functions"
      ],
      "metadata": {
        "id": "DsyaPgNr9obf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be a continuously differentiable, convex function. If $\\nabla f(\\mathbf{x}_0) = 0$, then $\\mathbf{x}_0$ is a global minimizer."
      ],
      "metadata": {
        "id": "EolBGSrX9oT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.2.7 - Any Local Minimizer of Convex Functions are Global Minimizers"
      ],
      "metadata": {
        "id": "whVlePTk9oLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be a convex function. Then, any local minimizer of $f$ is also a global minimizer."
      ],
      "metadata": {
        "id": "5yxTHpmh9n2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3.3 - Gradient Descent**"
      ],
      "metadata": {
        "id": "l9X9TynlrxV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemma 3.3.3.1 - Steepest Descent"
      ],
      "metadata": {
        "id": "dJaiWPbQtEPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be continuously differentiable at $\\mathbf{x}_0$. For any unit vector $\\mathbf{v} \\in \\mathbb{R}^d$,\n",
        "\\begin{equation*}\n",
        "  \\frac{\\partial f(\\mathbf{x}_0)}{\\partial \\mathbf{v}} \\geq \\frac{\\partial f(\\mathbf{x}_0)}{\\partial \\mathbf{v}^*}\n",
        "\\end{equation*}\n",
        "where\n",
        "\\begin{equation*}\n",
        "   \\mathbf{v}^* = -\\frac{\\nabla f(\\mathbf{x}_0)}{||\\nabla f(\\mathbf{x}_0)||}\n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "MiNHeq4J9sO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Theorem 3.3.3.2 - Step Size to Minimize Steepest Descent"
      ],
      "metadata": {
        "id": "iXV7AZv59svn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suupose that $f: \\mathbb{R}^d \\to \\mathbb{R}$ is twice continuously differentiable. The step size is chosen to minimize\n",
        "\\begin{equation*}\n",
        "  \\alpha_k = \\text{arg }\\min_{\\alpha > 0} f(\\mathbf{x}^k - \\alpha\\nabla f(\\mathbf{x}^k))\n",
        "\\end{equation*}\n",
        "Then, the steepest descent started from any $\\mathbf{x}^0$ produces a sequence $\\mathbf{x}^k$, $k = 1, 2, \\ldots$ such that if $\\nabla f(\\mathbf{x^k}) \\neq 0$, then\n",
        "\\begin{equation*}\n",
        "  f(\\mathbf{x}^{k+  1}) \\leq f(\\mathbf{x}^k)\n",
        "\\end{equation*}\n",
        "for all $k \\geq 1$."
      ],
      "metadata": {
        "id": "3k2tuRWD9sef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3.4 - References**"
      ],
      "metadata": {
        "id": "0tPzrdtDr14k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. MAT 494 Chapter 3 Notes"
      ],
      "metadata": {
        "id": "Pwf9Ulw-r31J"
      }
    }
  ]
}